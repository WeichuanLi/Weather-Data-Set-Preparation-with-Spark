{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Problem Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project is for preparing data set for weather observation for the year 2017. We are given a raw data set contains weather station, weather date, weather type and weather data corresponding to the weather type. We assume that the weather data is observed many times within a day and each weather station. In this way, we could group the data based on stations and dates and obtain the weather data with different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specification of the infrastructure that you have used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I used my PC as a Spark Standalone instance with HDFS & Hive to solve this problem. Python in the language to solve the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 26 2018 08:42:37)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"],'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://anshengs-mbp.attlocal.net:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Download the file to local directory and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file \n",
    "url=\"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2017.csv.gz\"\n",
    "filename = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file to local directory\n",
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('2017.csv.gz', 'rb') as f_in:\n",
    "    with open('2017.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)  Store the raw data in Hive as a table named ‘WeatherRaw’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# load data into spark RDD and save into HIVE table\n",
    "csv_data = sc.textFile(\"2017.csv\")\n",
    "csv_data  = csv_data.map(lambda p: p.split(\",\"))\n",
    "df_csv = csv_data.map(lambda p: Row(station_identifier = p[0], observation_date = p[1], observation_type=p[2], observation_value=p[3], observation_measure=p[4],observation_quality=p[5],observation_source=p[6],observation_time=p[7])).toDF()\n",
    "hc = HiveContext(sc)\n",
    "df_csv.write.format(\"orc\").saveAsTable(\"WeatherRaw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+-------------------+------------------+----------------+----------------+-----------------+------------------+\n",
      "|observation_date|observation_measure|observation_quality|observation_source|observation_time|observation_type|observation_value|station_identifier|\n",
      "+----------------+-------------------+-------------------+------------------+----------------+----------------+-----------------+------------------+\n",
      "|        20170101|                   |                   |                 N|                |            PRCP|                0|       US1MISW0005|\n",
      "|        20170101|                   |                   |                 N|                |            SNOW|                0|       US1MISW0005|\n",
      "|        20170101|                   |                   |                 N|                |            SNWD|                0|       US1MISW0005|\n",
      "|        20170101|                   |                   |                 N|                |            PRCP|                0|       CA1MB000296|\n",
      "|        20170101|                   |                   |                 a|                |            TMAX|              274|       ASN00015643|\n",
      "+----------------+-------------------+-------------------+------------------+----------------+----------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Generate the final dataset and store it in Hive as a table named ‘WeatherCurated’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Generate the \"WeatherCurated\" Table by doing data transformation. \n",
    "\n",
    "df_cur = df_csv.groupBy(\"station_identifier\",\"observation_date\").agg(\n",
    "(F.sum(F.when(F.col('observation_type')=='PRCP',F.col('observation_value')))/10).alias('Precipitation'),\n",
    "(F.max(F.when(F.col('observation_type')=='TMAX',F.col('observation_value')))/10).alias('MaxTemparature'),\n",
    "(F.min(F.when(F.col('observation_type')=='TMIN',F.col('observation_value')))/10).alias('MinTemparature'),\n",
    "(F.sum(F.when(F.col('observation_type')=='SNOW',F.col('observation_value')))).alias('Snowfall'),\n",
    "(F.sum(F.when(F.col('observation_type')=='SNWD',F.col('observation_value')))).alias('SnowDepth'),\n",
    "(F.sum(F.when(F.col('observation_type')=='EVAP',F.col('observation_value')))/10).alias('Evaporation'),\n",
    "(F.sum(F.when(F.col('observation_type')=='WESD',F.col('observation_value')))/10).alias('WaterEquivalentSnowDepth'),\n",
    "(F.sum(F.when(F.col('observation_type')=='WESF',F.col('observation_value')))/10).alias('WaterEquivalentSnowFall'),\n",
    "(F.sum(F.when(F.col('observation_type')=='PSUN',F.col('observation_value')))).alias('Sunshine')\n",
    ").sort('observation_date')\n",
    "\n",
    "df_cur = df_cur.select(F.col('station_identifier').alias('Station_identifier'),\n",
    "                       F.col('observation_date').alias('Observation_date'),\n",
    "                        F.col('Precipitation'),\n",
    "                        F.col('MaxTemparature'),\n",
    "                        F.col('MinTemparature'),\n",
    "                        F.col('Snowfall'),\n",
    "                        F.col('SnowDepth'),\n",
    "                        F.col('Evaporation'),\n",
    "                        F.col('WaterEquivalentSnowDepth'),\n",
    "                        F.col('WaterEquivalentSnowFall'),\n",
    "                        F.col('Sunshine')\n",
    "                        )\n",
    "df_cur.write.format(\"orc\").saveAsTable(\"WeatherCurated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-------------+--------------+--------------+--------+---------+-----------+------------------------+-----------------------+--------+\n",
      "|Station_identifier|Observation_date|Precipitation|MaxTemparature|MinTemparature|Snowfall|SnowDepth|Evaporation|WaterEquivalentSnowDepth|WaterEquivalentSnowFall|Sunshine|\n",
      "+------------------+----------------+-------------+--------------+--------------+--------+---------+-----------+------------------------+-----------------------+--------+\n",
      "|       ASN00011019|        20170101|          0.0|          22.1|          16.4|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00010519|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00003001|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00006020|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00009248|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00007124|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00012029|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00014173|        20170101|          0.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00014605|        20170101|         78.8|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "|       ASN00014732|        20170101|          2.0|          null|          null|    null|     null|       null|                    null|                   null|    null|\n",
      "+------------------+----------------+-------------+--------------+--------------+--------+---------+-----------+------------------------+-----------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cur.write.option('delimiter','\\t').csv('aggre.tsv')\n",
    "df_cur.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
